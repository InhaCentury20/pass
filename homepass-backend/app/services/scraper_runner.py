from __future__ import annotations

import logging
import subprocess
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

from app.config import settings

logger = logging.getLogger(__name__)

BACKEND_DIR = Path(__file__).resolve().parents[2]
PROJECT_ROOT = BACKEND_DIR.parent
DEFAULT_SCRAPER_DIR = PROJECT_ROOT / "homepass-scraper"


class ScraperRunner:
    """Launches the Scrapy crawler in a background thread."""

    def __init__(self):
        self._lock = threading.Lock()
        self._is_running = False
        self._thread: Optional[threading.Thread] = None

    def is_running(self) -> bool:
        return self._is_running

    def start(self, start_board_id: Optional[int] = None, days_limit: Optional[int] = None) -> None:
        logger.info("=" * 80)
        logger.info("ğŸš€ Scraper Runner ì‹œì‘ ìš”ì²­")
        logger.info(f"   ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"   íŒŒë¼ë¯¸í„°: start_board_id={start_board_id}, days_limit={days_limit}")
        
        with self._lock:
            if self._is_running:
                logger.warning("âš ï¸ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ìŠ¤í¬ë˜í¼ê°€ ìˆìŠµë‹ˆë‹¤.")
                raise RuntimeError("Scraper is already running")
            self._is_running = True
            logger.info("âœ… ìŠ¤í¬ë˜í¼ ì‹œì‘ ê°€ëŠ¥ (ì ê¸ˆ íšë“)")

        thread = threading.Thread(
            target=self._run_scraper,
            kwargs={
                "start_board_id": start_board_id or settings.SCRAPER_START_BOARD_ID,
                "days_limit": days_limit or settings.SCRAPER_DAYS_LIMIT,
            },
            daemon=True,
        )
        thread.start()
        self._thread = thread
        logger.info(f"ğŸ§µ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘ë¨ (Thread ID: {thread.ident})")
        logger.info("=" * 80)

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #
    def _resolve_paths(self) -> tuple[Path, Path]:
        logger.info("ğŸ“ ê²½ë¡œ í™•ì¸ ì¤‘...")
        
        scraper_dir = Path(settings.SCRAPER_DIR or DEFAULT_SCRAPER_DIR).resolve()
        logger.info(f"   ìŠ¤í¬ë˜í¼ ë””ë ‰í† ë¦¬: {scraper_dir}")
        
        if not scraper_dir.exists():
            logger.error(f"âŒ ìŠ¤í¬ë˜í¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {scraper_dir}")
            raise FileNotFoundError(f"Scraper directory not found: {scraper_dir}")
        logger.info(f"   âœ… ìŠ¤í¬ë˜í¼ ë””ë ‰í† ë¦¬ í™•ì¸ë¨")

        if settings.SCRAPER_VENV_PYTHON:
            python_path = Path(settings.SCRAPER_VENV_PYTHON).expanduser()
        else:
            candidate = scraper_dir / "venv" / "bin" / "python"
            python_path = candidate if candidate.exists() else Path("python")
        
        logger.info(f"   Python ê²½ë¡œ: {python_path}")
        logger.info(f"   Python ì¡´ì¬ ì—¬ë¶€: {python_path.exists() if python_path != Path('python') else 'system python'}")

        return scraper_dir, python_path

    def _run_scraper(self, start_board_id: int, days_limit: int) -> None:
        start_time = time.time()
        logger.info("=" * 80)
        logger.info("ğŸ”§ Scraper Pipeline ì‹¤í–‰ ì‹œì‘")
        logger.info(f"   start_board_id: {start_board_id}")
        logger.info(f"   days_limit: {days_limit}")
        logger.info("=" * 80)
        
        try:
            scraper_dir, python_path = self._resolve_paths()
            
            # Step 1: SOCO Spider
            logger.info("=" * 80)
            logger.info("ğŸ“¡ [1/2] SOCO Spider ì‹¤í–‰")
            logger.info("=" * 80)
            step1_start = time.time()
            self._run_soco_spider(scraper_dir, python_path, start_board_id, days_limit)
            step1_elapsed = time.time() - step1_start
            logger.info(f"âœ… [1/2] SOCO Spider ì™„ë£Œ (ì†Œìš” ì‹œê°„: {step1_elapsed:.2f}ì´ˆ)")
            
            # LH importerëŠ” ì œì™¸ (ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ)
            # self._run_lh_import(scraper_dir, python_path)
            
            # Step 2: Extractor
            logger.info("=" * 80)
            logger.info("ğŸ”¬ [2/2] New Extractor ì‹¤í–‰")
            logger.info("=" * 80)
            step2_start = time.time()
            self._run_extractor(scraper_dir, python_path)
            step2_elapsed = time.time() - step2_start
            logger.info(f"âœ… [2/2] New Extractor ì™„ë£Œ (ì†Œìš” ì‹œê°„: {step2_elapsed:.2f}ì´ˆ)")
            
            total_elapsed = time.time() - start_time
            logger.info("=" * 80)
            logger.info("ğŸ‰ Scraper Pipeline ì „ì²´ ì™„ë£Œ!")
            logger.info(f"   ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ")
            logger.info(f"   ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info("=" * 80)
            
        except Exception as exc:  # noqa: BLE001
            elapsed = time.time() - start_time
            logger.error("=" * 80)
            logger.exception("ğŸ’¥ Scraper pipeline failed (ì†Œìš” ì‹œê°„: %.2fì´ˆ): %s", elapsed, exc)
            logger.error("=" * 80)
        finally:
            with self._lock:
                self._is_running = False
                logger.info("ğŸ”“ ìŠ¤í¬ë˜í¼ ì ê¸ˆ í•´ì œë¨")

    def _run_soco_spider(self, scraper_dir: Path, python_path: Path, start_board_id: int, days_limit: int) -> None:
        cmd = [
            str(python_path),
            "-m",
            "scrapy",
            "crawl",
            "soco_board_spider",
            "-a",
            f"start_board_id={start_board_id}",
            "-a",
            f"days_limit={days_limit}",
        ]
        self._run_subprocess(cmd, scraper_dir, "SOCO spider")

    def _run_lh_import(self, scraper_dir: Path, python_path: Path) -> None:
        lh_script = scraper_dir / "Lh.py"
        if not lh_script.exists():
            logger.warning("LH importer script not found at %s; skipping", lh_script)
            return
        cmd = [str(python_path), str(lh_script)]
        self._run_subprocess(cmd, scraper_dir, "LH importer")

    def _run_extractor(self, scraper_dir: Path, python_path: Path) -> None:
        extractor_script = scraper_dir / "new_extractor.py"
        logger.info(f"ğŸ“„ Extractor ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ: {extractor_script}")
        
        if not extractor_script.exists():
            logger.warning("âš ï¸ Extractor script not found at %s; skipping", extractor_script)
            return
        
        logger.info("âœ… Extractor ìŠ¤í¬ë¦½íŠ¸ í™•ì¸ë¨")
        cmd = [str(python_path), str(extractor_script)]
        self._run_subprocess(cmd, scraper_dir, "New extractor")

    def _run_subprocess(self, cmd: list[str], cwd: Path, label: str) -> None:
        logger.info("-" * 80)
        logger.info(f"â–¶ï¸  {label} ì‹œì‘")
        logger.info(f"   ì‘ì—… ë””ë ‰í† ë¦¬: {cwd}")
        logger.info(f"   ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
        logger.info("-" * 80)
        
        start_time = time.time()
        try:
            result = subprocess.run(
                cmd, 
                cwd=str(cwd), 
                check=True,
                capture_output=True,
                text=True
            )
            elapsed = time.time() - start_time
            
            logger.info("-" * 80)
            logger.info(f"âœ… {label} ì„±ê³µ (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            
            # stdout ì¶œë ¥ (new_extractor.pyì˜ ë¡œê·¸)
            if result.stdout:
                logger.info(f"ğŸ“ {label} ì¶œë ¥:")
                for line in result.stdout.splitlines():
                    logger.info(f"   {line}")
            
            # stderrê°€ ìˆìœ¼ë©´ ê²½ê³ ë¡œ ì¶œë ¥
            if result.stderr:
                logger.warning(f"âš ï¸ {label} ê²½ê³ /ì—ëŸ¬ ì¶œë ¥:")
                for line in result.stderr.splitlines():
                    logger.warning(f"   {line}")
            
            logger.info("-" * 80)
            
        except subprocess.CalledProcessError as e:
            elapsed = time.time() - start_time
            logger.error("-" * 80)
            logger.error(f"âŒ {label} ì‹¤íŒ¨ (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
            logger.error(f"   ì¢…ë£Œ ì½”ë“œ: {e.returncode}")
            
            if e.stdout:
                logger.error(f"   í‘œì¤€ ì¶œë ¥:")
                for line in e.stdout.splitlines():
                    logger.error(f"      {line}")
            
            if e.stderr:
                logger.error(f"   ì—ëŸ¬ ì¶œë ¥:")
                for line in e.stderr.splitlines():
                    logger.error(f"      {line}")
            
            logger.error("-" * 80)
            raise


scraper_runner = ScraperRunner()


