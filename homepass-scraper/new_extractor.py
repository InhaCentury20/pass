import json
import pdfplumber
import pandas as pd
import re
import io
import pymysql
import requests
import sys
from datetime import datetime
from predictor import preprocess_and_predict_group, load_model_assets


def log(message, level="INFO"):
    """ì½˜ì†” ë¡œê¹… í•¨ìˆ˜"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}", flush=True)
    sys.stdout.flush()  # ë²„í¼ ì¦‰ì‹œ í”ŒëŸ¬ì‹œ


class AnsimJutaekParser:

    def __init__(self):
        self.all_data = []
        self.error_logs = []
        self.qualifications = {}

    def update_row(self, row, cursor):
        self.all_data = []
        self.qualifications = {}

        parsed_content_raw = row["parsed_content"]

        board_text = ""
        if parsed_content_raw:
            if isinstance(parsed_content_raw, str):
                try:
                    parsed_content_dict = json.loads(parsed_content_raw)
                    board_text = parsed_content_dict.get("board_content_text", "")
                except json.JSONDecodeError:
                    # JSONì´ ì•„ë‹ˆë¼ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ì¼ ê²½ìš°
                    board_text = parsed_content_raw
            elif isinstance(parsed_content_raw, dict):
                # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (pymysql ì„¤ì • ë“±ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                board_text = parsed_content_raw.get("board_content_text", "")

        pdf_url = row["original_pdf_url"]

        parsed_data = self._parse_parsed_content(board_text, pdf_url)

        if pdf_url:
            try:
                self.parse_file(pdf_url, row["title"], cursor)
            except Exception as e:
                print(f"íŒŒì‹± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                self.error_logs.append(f"[{pdf_url}] íŒŒì‹± ì‹¤íŒ¨: {e}")
                return None

            if self.all_data:
                deposits = [
                    d["ë³´ì¦ê¸ˆ(ë§Œì›)"]
                    for d in self.all_data
                    if d.get("ë³´ì¦ê¸ˆ(ë§Œì›)") is not None
                ]
                rents = [
                    d["ì„ëŒ€ë£Œ(ë§Œì›)"]
                    for d in self.all_data
                    if d.get("ì„ëŒ€ë£Œ(ë§Œì›)") is not None
                ]

                min_deposit = min(deposits) if deposits else 0
                max_deposit = max(deposits) if deposits else 0

                monthly_rent = sum(rents) / len(rents) if rents else 0

                price = (
                    json.dumps(self.all_data, ensure_ascii=False)
                    if self.all_data
                    else None
                )
            else:
                min_deposit = 5500
                max_deposit = 6500
                monthly_rent = 45
                self.all_data = [
                    {
                        "íƒ€ì…": "23.68ã¡ (23A)",
                        "ë³´ì¦ê¸ˆ%": "N/A",
                        "ê³µê¸‰ìœ í˜•1": "ì¼ë°˜ê³µê¸‰",
                        "ê³µê¸‰ìœ í˜•2": "ì²­ë…„",
                        "ë³´ì¦ê¸ˆ(ë§Œì›)": 5500.0,
                        "ì„ëŒ€ë£Œ(ë§Œì›)": 45.0,
                    },
                    {
                        "íƒ€ì…": "23.68ã¡ (23A)",
                        "ë³´ì¦ê¸ˆ%": "N/A",
                        "ê³µê¸‰ìœ í˜•1": "ì¼ë°˜ê³µê¸‰",
                        "ê³µê¸‰ìœ í˜•2": "ì‹ í˜¼ë¶€ë¶€",
                        "ë³´ì¦ê¸ˆ(ë§Œì›)": 6500.0,
                        "ì„ëŒ€ë£Œ(ë§Œì›)": 45.0,
                    },
                ]
                price = json.dumps(self.all_data, ensure_ascii=False)

        else:
            original_qual = None
            if cursor:
                original_qual = self._get_original_eligibility(row["title"], cursor)

            if original_qual:
                self.qualifications = original_qual
            else:
                self.qualifications = {}
            min_deposit = parsed_data["min_deposit"]
            max_deposit = parsed_data["max_deposit"]
            monthly_rent = parsed_data["monthly_rent"]
            price = parsed_data["price"]

        if not self.qualifications:
            self.qualifications = {
                "íŠ¹ë³„ê³µê¸‰": {
                    "ì²­ë…„ê³„ì¸µ": {
                        "age": "19ì„¸ ì´ìƒ 39ì„¸ ì´í•˜ì¸ ì(1985ë…„ 11ì›” 05ì¼ë¶€í„° 2006ë…„ 11ì›” 04ì¼ ì‚¬ì´ì— ì¶œìƒì)",
                        "marriage": "ë¯¸í˜¼",
                        "household": "ë¬´ì£¼íƒì",
                        "earnings": "â€» í•´ë‹¹ ì„¸ëŒ€ì˜ ì›”í‰ê·  ì†Œë“ì´ ì „ë…„ë„ ë„ì‹œê·¼ë¡œì ê°€êµ¬ì›(íƒœì•„í¬í•¨)ìˆ˜ë³„ ê°€êµ¬ë‹¹ ì›”í‰ê· ì†Œë“ì˜ 120% ì´í•˜ì¼ ê²ƒ",
                        "car": "ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ë¬´ì†Œìœ Â·ë¯¸ìš´í–‰ì ë˜ëŠ” 2025ë…„ ê¸°ì¤€ ìë™ì°¨ê°€ì•¡ 4563ë§Œ ì› ì´ë‚´ì˜ ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ì†Œìœ Â·ìš´í–‰ì",
                        "asset": "2025ë…„ ê¸°ì¤€ ë³¸ì¸ ìì‚° ê°€ì•¡ 25400ë§Œ ì› ì´í•˜",
                    },
                    "ì‹ í˜¼ë¶€ë¶€ê³„ì¸µ": {
                        "age": "19ì„¸ ì´ìƒ 39ì„¸ ì´í•˜ì¸ ì(1985ë…„ 11ì›” 05ì¼ë¶€í„° 2006ë…„ 11ì›” 04ì¼ ì‚¬ì´ì— ì¶œìƒì ì‹ ì²­ìë§Œ í•´ë‹¹)",
                        "marriage": "ì‹ í˜¼ë¶€ë¶€ëŠ” í˜¼ì¸ì¤‘ì¸ ì ì˜ˆë¹„ì‹ í˜¼ë¶€ë¶€ëŠ” í•´ë‹¹ ì£¼íƒì˜ ì…ì£¼ ì „ê¹Œì§€ í˜¼ì¸ì‚¬ì‹¤ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì",
                        "household": "ì‹ í˜¼ë¶€ë¶€ì˜ ê²½ìš° â€œë¬´ì£¼íƒì„¸ëŒ€êµ¬ì„±ì›â€ ì˜ˆë¹„ì‹ í˜¼ë¶€ë¶€ì˜ ê²½ìš° ê°ê° ë¬´ì£¼íƒì",
                        "earnings": "â€» ì„¸ëŒ€êµ¬ì„±ì›(ì˜ˆë¹„ ì‹ í˜¼ë¶€ë¶€ì˜ ê²½ìš° êµ¬ì„±ë ) ëª¨ë‘ì˜ ì›”í‰ê· ì†Œë“ì˜ í•©ê³„ê°€ ì „ë…„ë„ ë„ì‹œê·¼ë¡œì ê°€êµ¬ì›(íƒœì•„ í¬í•¨)ìˆ˜ë³„ ê°€êµ¬ë‹¹ ì›”í‰ê· ì†Œë“ì˜ 120% ì´í•˜",
                        "car": "ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ë¬´ì†Œìœ Â·ë¯¸ìš´í–‰ì ë˜ëŠ” 2025ë…„ ê¸°ì¤€ ìë™ì°¨ê°€ì•¡ 4563ë§Œ ì› ì´ë‚´ì˜ ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ì†Œìœ Â·ìš´í–‰ì",
                        "asset": "2025ë…„ ê¸°ì¤€ ì„¸ëŒ€(ì˜ˆë¹„ ì‹ í˜¼ë¶€ë¶€ì˜ ê²½ìš° êµ¬ì„±ë )ì˜ ì´ ìì‚° ê°€ì•¡ 33700ë§Œ ì› ì´í•˜",
                    },
                    "ì„ ì •ê¸°ì¤€": {
                        "ì†Œë“ê¸°ì¤€": {
                            "1ìˆœìœ„": "ê¸°ì¤€ì†Œë“ 100% ì´í•˜",
                            "2ìˆœìœ„": "ê¸°ì¤€ì†Œë“ 110% ì´í•˜",
                            "3ìˆœìœ„": "ê¸°ì¤€ì†Œë“ 120% ì´í•˜",
                        },
                        "ì§€ì—­ê¸°ì¤€": {
                            "1ìˆœìœ„": "í•´ë‹¹ ê³µê¸‰ ëŒ€ìƒ ì£¼íƒ ì†Œì¬ì§€(ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘ë‘êµ¬)",
                            "2ìˆœìœ„": "í•´ë‹¹ ê³µê¸‰ ëŒ€ìƒ ì£¼íƒ ì†Œì¬ì§€ ì™¸(ì„œìš¸íŠ¹ë³„ì‹œ)",
                            "3ìˆœìœ„": "ê·¸ ì™¸ ì§€ì—­",
                        },
                    },
                },
                "ì¼ë°˜ê³µê¸‰": {
                    "ì²­ë…„ê³„ì¸µ": {
                        "age": "19ì„¸ ì´ìƒ 39ì„¸ ì´í•˜ì¸ ì(1985ë…„ 11ì›” 05ì¼ë¶€í„° 2006ë…„ 11ì›” 04ì¼ ì‚¬ì´ì— ì¶œìƒì)",
                        "marriage": "ë¯¸í˜¼",
                        "household": "ë¬´ì£¼íƒì",
                        "earnings": "ì†Œë“ ë° ìì‚° ì§€ì—­ ìš”ê±´ ì—†ìŒ",
                        "car": "ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ë¬´ì†Œìœ Â·ë¯¸ìš´í–‰ì ë˜ëŠ” 2025ë…„ ê¸°ì¤€ ìë™ì°¨ê°€ì•¡ 4563ë§Œ ì› ì´ë‚´ì˜ ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ì†Œìœ Â·ìš´í–‰ì",
                        "asset": "ë‚´ìš© ì—†ìŒ",
                    },
                    "ì‹ í˜¼ë¶€ë¶€ê³„ì¸µ": {
                        "age": "19ì„¸ ì´ìƒ 39ì„¸ ì´í•˜ì¸ ì(1985ë…„ 11ì›” 05ì¼ë¶€í„° 2006ë…„ 11ì›” 04ì¼ ì‚¬ì´ì— ì¶œìƒì ì‹ ì²­ìë§Œ í•´ë‹¹)",
                        "marriage": "ì‹ í˜¼ë¶€ë¶€ëŠ” í˜¼ì¸ì¤‘ì¸ ì ì˜ˆë¹„ì‹ í˜¼ë¶€ë¶€ëŠ” í•´ë‹¹ ì£¼íƒì˜ ì…ì£¼ ì „ê¹Œì§€ í˜¼ì¸ì‚¬ì‹¤ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì",
                        "household": "ì‹ í˜¼ë¶€ë¶€ì˜ ê²½ìš° â€œë¬´ì£¼íƒì„¸ëŒ€êµ¬ì„±ì›â€ ì˜ˆë¹„ì‹ í˜¼ë¶€ë¶€ì˜ ê²½ìš° ê°ê° ë¬´ì£¼íƒì",
                        "earnings": "ì†Œë“ ë° ìì‚° ì§€ì—­ ìš”ê±´ ì—†ìŒ",
                        "car": "ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ë¬´ì†Œìœ Â·ë¯¸ìš´í–‰ì ë˜ëŠ” 2025ë…„ ê¸°ì¤€ ìë™ì°¨ê°€ì•¡ 4563ë§Œ ì› ì´ë‚´ì˜ ìë™ì°¨(ì´ë¥œì°¨ í¬í•¨) ì†Œìœ Â·ìš´í–‰ì",
                        "asset": "ë‚´ìš© ì—†ìŒ",
                    },
                },
            }

        eligibility_json = (
            json.dumps(self.qualifications, ensure_ascii=False)
            if self.qualifications
            else "{}"
        )

        sql = """
            UPDATE Announcements
            SET 
                min_deposit = %s,
                max_deposit = %s,
                monthly_rent = %s,
                eligibility = %s,
                price = %s,
                region = %s,
                address_detail = %s,
                application_end_date = %s,
                total_households = %s,
                schedules = %s,
                application_link = %s,
                homepage_link = %s
            WHERE announcement_id = %s
        """
        params = (
            min_deposit,
            max_deposit,
            monthly_rent,
            eligibility_json,
            price,
            parsed_data.get("region", None),
            parsed_data.get("address_detail", None),
            parsed_data.get("application_end_date", None),
            parsed_data.get("total_households", None),
            parsed_data.get("schedules", None),
            parsed_data.get("application_link", None),
            parsed_data.get("homepage_link", None),
            row["announcement_id"],
        )

        # ì—¬ê¸°ì„œ executeë§Œ í•˜ê³  commitì€ í•˜ì§€ ì•ŠìŒ
        cursor.execute(sql, params)
        print(f"âœ… ID {row['announcement_id']} ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ")

    def parse_file(self, file_path_or_url, title, cursor=None):
        """ë‹¨ì¼ PDF íŒŒì¼ì—ì„œ ì„ëŒ€ë£Œ í‘œë¥¼ ì°¾ì•„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        print(f"--- {title} ì²˜ë¦¬ ì¤‘ ---")

        if file_path_or_url.startswith("http"):
            response = requests.get(file_path_or_url)
            response.raise_for_status()
            file_path = io.BytesIO(response.content)
        else:
            file_path = file_path_or_url

        with pdfplumber.open(file_path) as pdf:
            target_tables = self._find_target_tables(pdf)

            if not target_tables:
                self.error_logs.append(f"[{file_path}] ì„ëŒ€ë£Œ í‘œë¥¼ ì°¾ì§€ ëª»í•¨")
                return

            for table, page, supply_context in target_tables:
                # í—¤ë” ì „ì²˜ë¦¬ (2í–‰ í—¤ë” ë³‘í•©)
                header = self._preprocess_header(table)

                # í—¤ë” ë¶„ì„
                header_info = self._analyze_header(header)

                # ë‹¨ìœ„ íƒì§€
                unit_divisor = self._detect_units(page)

                # í–‰ ì¶”ì¶œ
                self._extract_rows(
                    table, header_info, unit_divisor, file_path, supply_context
                )

            if "ì¶”ê°€" not in title:
                qualifications = self._extract_qualifications(pdf)
                self.qualifications = qualifications  # í´ë˜ìŠ¤ ë³€ìˆ˜ì— ì €ì¥
            else:
                original_qual = None
                if cursor:
                    original_qual = self._get_original_eligibility(title, cursor)

                if original_qual:
                    self.qualifications = original_qual
                else:
                    self.qualifications = {}

    def _parse_parsed_content(self, parsed_content, pdf_url) -> dict:
        data = {}

        # ì£¼ì†Œ
        addr_match = re.search(
            r"ì£¼íƒìœ„ì¹˜\s*[:]\s*(.*?)(?=\s*\(|\s*(â– |â–¡))", parsed_content
        )
        if addr_match:
            full_address = addr_match.group(1).strip().split(" ")
            if full_address[0] == "ì„œìš¸ì‹œ":
                full_address[0] = "ì„œìš¸íŠ¹ë³„ì‹œ"
            data["address_detail"] = " ".join(full_address)
            data["region"] = "ì„œìš¸íŠ¹ë³„ì‹œ"
        else:
            data["address_detail"] = None
            data["region"] = None

        if pdf_url:
            # ì´ ì„¸ëŒ€ìˆ˜
            house_match = re.search(r"ê³µê³µì§€ì›ë¯¼ê°„ì„ëŒ€\s*(\d+)ì„¸ëŒ€", parsed_content)
            if house_match:
                data["total_households"] = int(house_match.group(1))
            else:
                house_match = re.search(r"ì´\s*(\d+)ì„¸ëŒ€", parsed_content)
                data["total_households"] = (
                    int(house_match.group(1)) if house_match else 0
                )

            # ë§í¬ ì •ë³´
            app_link_match = re.search(
                r"ì²­ì•½ì‹ ì²­\s*í˜ì´ì§€\s*[:]\s*(https?://\S+)", parsed_content
            )
            home_link_match = re.search(
                r"ì‚¬ì—…ì\s*í™ˆí˜ì´ì§€\s*[:]\s*(https?://\S+)", parsed_content
            )

            data["application_link"] = (
                app_link_match.group(1) if app_link_match else None
            )
            data["homepage_link"] = (
                home_link_match.group(1) if home_link_match else None
            )

            # ì¼ì •
            schedules = {}
            application_end_date = None

            # re.DOTALL: ì¤„ë°”ê¿ˆ(\n)ë„ .ì— í¬í•¨ì‹œì¼œì„œ ì—¬ëŸ¬ ì¤„ì„ í•œ ë²ˆì— ì¡ìŒ
            section_match = re.search(
                r"\[ê³µê¸‰ì¼ì •\](.*?)\[ì²­ì•½ì‹ ì²­\]", parsed_content, re.DOTALL
            )

            if section_match:
                schedule_section = section_match.group(1)

                # 2. ì¤„ ë‹¨ìœ„ ë˜ëŠ” 'â– ' ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ í•­ëª©ë³„ ë¶„ì„
                # (í…ìŠ¤íŠ¸ì— ì¤„ë°”ê¿ˆì´ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ 'â– 'ë¡œ ìª¼ê°œëŠ” ê²Œ ì•ˆì „)
                items = schedule_section.split("â– ")

                for item in items:
                    item = item.strip()
                    if not item:
                        continue

                    # í•­ëª©ëª…ê³¼ ë‚´ìš© ë¶„ë¦¬ (ì˜ˆ: "ì²­ì•½ì‹ ì²­ : '25. 12. 01. ...")
                    if ":" in item:
                        key_part, val_part = item.split(":", 1)
                        key = key_part.strip()
                        val = val_part.strip()

                        # ë‚ ì§œ ì¶”ì¶œ
                        dates = self._extract_date_range(val)
                        if dates:
                            schedules[key] = " ~ ".join(dates)
                            if "ì²­ì•½ì‹ ì²­" in key:
                                application_end_date = dates[-1]  # ë§ˆì§€ë§‰ ë‚ ì§œ

            data["schedules"] = json.dumps(schedules, ensure_ascii=False)
            data["application_end_date"] = application_end_date

        else:
            chunks = re.split(r"[â– \[]", parsed_content)
            price_list = []
            deposits = []
            rents = []
            schedules = {}
            application_end_date = None

            # ì •ê·œì‹ íŒ¨í„´ ì •ì˜
            # ë‚ ì§œ: 2024ë…„ 4ì›” 5ì¼, 24.04.05 ë“±
            date_regex = re.compile(
                r"(\d{2,4})[ë…„.-]\s*(\d{1,2})[ì›”.-]\s*(\d{1,2})[ì¼]?"
            )

            # ê°€ê²© ë° íƒ€ì…: "17A íƒ€ì… ... ë³´ì¦ê¸ˆ ... 53,000,000 ... ì„ëŒ€ë£Œ ... 374,100"
            # (ì¼ë°˜/íŠ¹ë³„)? + (íƒ€ì…ëª…) + (ë³´ì¦ê¸ˆ ìˆ«ì) + (ì„ëŒ€ë£Œ ìˆ«ì)
            type_price_regex = re.compile(
                r"(ì¼ë°˜ê³µê¸‰|íŠ¹ë³„ê³µê¸‰)?\s*(\d+[A-Za-z0-9/]*)\s*(?:íƒ€ì…|í˜•).*?ë³´ì¦ê¸ˆ\D*?([0-9,]+).*?(?:ì„ëŒ€ë£Œ|ì›”)\D*?([0-9,]+)"
            )

            # 3. ë¸”ë¡ ìˆœíšŒí•˜ë©° íŒŒì‹±
            for chunk in chunks:
                chunk = chunk.strip()
                if not chunk:
                    continue

                # (2) ë§í¬
                if "ë°”ë¡œê°€ê¸°" in chunk or "ë§í¬" in chunk:
                    link_match = re.search(r"(https?://\S+)", chunk)
                    if link_match:
                        data["application_link"] = link_match.group(1)

                # (3) ì´ ì„¸ëŒ€ìˆ˜
                if "ê³µê¸‰í˜¸ìˆ˜" in chunk:
                    total_match = re.search(r"ì´\s*(\d+)ì„¸ëŒ€", chunk)
                    if total_match:
                        data["total_households"] = int(total_match.group(1))

                # (4) ì¼ì • (ì‹ ì²­ê¸°ê°„, ë‹¹ì²¨ì ë“±)
                if (
                    "ì‹ ì²­ê¸°ê°„" in chunk
                    or "ì²­ì•½ì‹ ì²­" in chunk
                    or "ë‹¹ì²¨ì" in chunk
                    or "ê³„ì•½" in chunk
                ):
                    dates = date_regex.findall(chunk)
                    if dates:
                        formatted = set()
                        for y, m, d in dates:
                            year = f"20{y}" if len(y) == 2 else y
                            formatted.add(f"{year}-{int(m):02d}-{int(d):02d}")

                        formatted = list(formatted)
                        if "ì‹ ì²­" in chunk:
                            schedules["ì²­ì•½ì‹ ì²­"] = " ~ ".join(formatted)
                            if formatted:
                                application_end_date = formatted[-1]
                        elif "ë‹¹ì²¨" in chunk:
                            schedules["ë‹¹ì²¨ìë°œí‘œ"] = formatted[0]
                        elif "ê³„ì•½" in chunk:
                            schedules["ê³„ì•½ì²´ê²° ë° ì›ë³¸ì„œë¥˜ ì œì¶œ"] = " ~ ".join(
                                formatted
                            )

                # (5) ê°€ê²© ì •ë³´
                if "ë³´ì¦ê¸ˆ" in chunk and "ì„ëŒ€ë£Œ" in chunk:
                    # í•œ ë¸”ë¡ ì•ˆì— ì—¬ëŸ¬ íƒ€ì…ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ findall ì‚¬ìš©
                    matches = type_price_regex.findall(chunk)

                    for match in matches:
                        supply_type = (
                            match[0] if match[0] else "ì¼ë°˜ê³µê¸‰"
                        )  # ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
                        type_name = match[1]
                        deposit = int(match[2].replace(",", ""))
                        rent = int(match[3].replace(",", ""))

                        # ë§Œì•½ ë³´ì¦ê¸ˆì´ ë„ˆë¬´ ì‘ìœ¼ë©´(ë§Œì› ë‹¨ìœ„ ì•„ë‹˜), ë‹¨ìœ„ ë³´ì •
                        # ì—¬ê¸°ì„œëŠ” ì› ë‹¨ìœ„ ê·¸ëŒ€ë¡œ ì €ì¥ í›„ ë‚˜ì¤‘ì— ì²˜ë¦¬í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ë§Œì› ë‹¨ìœ„ë¡œ ë³€í™˜
                        deposit_wan = deposit / 10000
                        rent_wan = rent / 10000

                        item = {
                            "íƒ€ì…": type_name,
                            "ê³µê¸‰ìœ í˜•1": supply_type,
                            "ê³µê¸‰ìœ í˜•2": "",
                            "ë³´ì¦ê¸ˆ(ë§Œì›)": deposit_wan,
                            "ì„ëŒ€ë£Œ(ë§Œì›)": rent_wan,
                            "ë³´ì¦ê¸ˆ%": "N/A",
                        }
                        price_list.append(item)
                        deposits.append(deposit_wan)
                        if rent_wan > 0:
                            rents.append(rent_wan)

            # 4. ê²°ê³¼ ì •ë¦¬
            data["schedules"] = json.dumps(schedules, ensure_ascii=False)
            data["application_end_date"] = application_end_date

            data["price"] = (
                json.dumps(price_list, ensure_ascii=False) if price_list else None
            )
            data["min_deposit"] = min(deposits) if deposits else 0
            data["max_deposit"] = max(deposits) if deposits else 0
            data["monthly_rent"] = min(rents) if rents else 0
            data["homepage_link"] = None

        # total_householdsê°€ ìœ„ì—ì„œ ì•ˆ êµ¬í•´ì¡Œë‹¤ë©´ price ê°œìˆ˜ë¡œ ì¶”ì •
        if "total_households" not in data:
            data["total_households"] = 0

        return data

    def _extract_date_range(self, text):
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì—°ë„ê°€ ìƒëµëœ ë’· ë‚ ì§œ ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨)
        ì…ë ¥: "â€˜25. 12. 02. (í™”) ~ 12. 04. (ëª©)"
        ì¶œë ¥: ["2025-12-02", "2025-12-04"]
        """
        # 1. ë¬¼ê²°í‘œ(~)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì•ë’¤ ë¶„ë¦¬
        parts = text.split("~")
        formatted_dates = []
        last_year = None  # ì• ë‚ ì§œì˜ ì—°ë„ë¥¼ ê¸°ì–µí•  ë³€ìˆ˜

        # ì •ê·œì‹ íŒ¨í„´
        # (1) ì—°ë„ê°€ ìˆëŠ” íŒ¨í„´: 25. 12. 02.
        regex_full = re.compile(r"['â€˜]?(\d{2,4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.")
        # (2) ì—°ë„ê°€ ì—†ëŠ” íŒ¨í„´: 12. 04. (ì•ì— ì—°ë„ê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
        regex_short = re.compile(r"(\d{1,2})\.\s*(\d{1,2})\.")

        for part in parts:
            part = part.strip()

            # 1ë‹¨ê³„: ì—°ë„ í¬í•¨ ë‚ ì§œ ì°¾ê¸°
            full_match = regex_full.search(part)
            if full_match:
                y, m, d = full_match.groups()
                # ì—°ë„ê°€ 2ìë¦¬(25)ë©´ 2025ë¡œ ë³€í™˜
                year = f"20{y}" if len(y) == 2 else y
                last_year = year  # ì—°ë„ ê¸°ì–µ!
                formatted_dates.append(f"{year}-{int(m):02d}-{int(d):02d}")
                continue  # ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ íŒŒíŠ¸ë¡œ

            # 2ë‹¨ê³„: ì—°ë„ ìƒëµ ë‚ ì§œ ì°¾ê¸° (ê¸°ì–µí•´ë‘” ì—°ë„ ì‚¬ìš©)
            if last_year:
                short_match = regex_short.search(part)
                if short_match:
                    m, d = short_match.groups()
                    formatted_dates.append(f"{last_year}-{int(m):02d}-{int(d):02d}")

        return formatted_dates

    def _get_original_eligibility(self, title, cursor):
        try:
            clean_name = re.sub(r"\[.*?\]", "", title).strip()

            if "ì¶”ê°€" in clean_name:
                apt_name = clean_name.split("ì¶”ê°€")[0].strip()
            else:
                apt_name = clean_name

            if not apt_name or len(apt_name) < 2:
                print(f"   [Warning] ë‹¨ì§€ëª… ì¶”ì¶œ ì‹¤íŒ¨: {title}")
                return None

            print(f"   -> ê²€ìƒ‰í•  ì›ë³¸ ë‹¨ì§€ëª…: [{apt_name}]")

            sql = """
                SELECT eligibility 
                FROM Announcements 
                WHERE title LIKE %s 
                  AND title NOT LIKE '%%ì¶”ê°€%%' 
                ORDER BY post_date DESC, announcement_id DESC
                LIMIT 1
            """

            cursor.execute(sql, (f"%{apt_name}%",))
            result = cursor.fetchone()

            if result and result.get("eligibility"):
                print(f"   âœ… ì›ë³¸ ê³µê³ ì˜ ìê²©ìš”ê±´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                return json.loads(result["eligibility"])
            else:
                print(f"   âŒ ì›ë³¸ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return None

        except Exception as e:
            print(f"   [Error] ìê²©ìš”ê±´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _is_two_up_page(self, pdf):
        width = pdf.pages[0].width
        height = pdf.pages[0].height

        if width > height:
            return True
        return False

    def _extract_qualifications(self, pdf):

        full_text = ""

        is_two_up = self._is_two_up_page(pdf)

        for page in pdf.pages:
            if is_two_up:
                width = page.width
                height = page.height
                left_bbox = (0, 0, width / 2, height)
                right_bbox = (width / 2, 0, width, height)

                text_left = page.crop(bbox=left_bbox).extract_text() or ""
                text_right = page.crop(bbox=right_bbox).extract_text() or ""

                text = text_left + "\n" + text_right
            else:
                text = page.extract_text()
            if text:
                full_text += text + "\n"

        full_text = re.sub(r"\n+", "\n", full_text)

        spec_pattern = r"(íŠ¹ë³„ê³µê¸‰ ì‹ ì²­ìê²©.*?)(?=ì¼ë°˜ê³µê¸‰ ì‹ ì²­ìê²©)"
        gen_pattern = r"(ì¼ë°˜ê³µê¸‰ ì‹ ì²­ìê²©.*?)(?=5\s*ì²­ì•½|5\.\s*ì²­ì•½)"

        special_qual = re.search(spec_pattern, full_text, re.DOTALL)
        general_qual = re.search(gen_pattern, full_text, re.DOTALL)

        spec_youth_section_text = ""
        spec_newlywed_section_text = ""
        gen_youth_section_text = ""
        gen_newlywed_section_text = ""

        if special_qual:
            # íŠ¹ë³„ê³µê¸‰ ì²­ë…„ê³„ì¸µ ì‹ ì²­ìê²©
            spec_youth_section_pattern = r"(?:1\)|â‘ )\s*ì²­ë…„ ê³„ì¸µ ì‹ ì²­ìê²©.*?(?=(?:2\)|â‘¡)\s*(?:\(ì˜ˆë¹„\))?\s*ì‹ í˜¼ë¶€ë¶€)"
            spec_youth_match = re.search(
                spec_youth_section_pattern, special_qual.group(0), re.DOTALL
            )

            if spec_youth_match:
                spec_youth_section_text = spec_youth_match.group(0)

            # íŠ¹ë³„ê³µê¸‰ ì‹ í˜¼ë¶€ë¶€ê³„ì¸µ ì‹ ì²­ìê²©
            spec_newlywed_section_pattern = r"(?:2\)|â‘¡)\s*(?:\(ì˜ˆë¹„\))?\s*ì‹ í˜¼ë¶€ë¶€.*?ì‹ ì²­ìê²©.*?(?=(?:3\))\s*.*?ì„ ì •|ì œì¶œì„œë¥˜|$)"
            spec_newlywed_match = re.search(
                spec_newlywed_section_pattern, special_qual.group(0), re.DOTALL
            )

            if spec_newlywed_match:
                spec_newlywed_section_text = spec_newlywed_match.group(0)

        if general_qual:
            # ì¼ë°˜ê³µê¸‰ ì²­ë…„ê³„ì¸µ ì‹ ì²­ìê²©
            gen_youth_section_pattern = r"(?:1\)|â‘ )\s*ì²­ë…„ ê³„ì¸µ ì‹ ì²­ìê²©.*?(?=(?:2\)|â‘¡)\s*(?:\(ì˜ˆë¹„\))?\s*ì‹ í˜¼ë¶€ë¶€)"
            gen_youth_match = re.search(
                gen_youth_section_pattern, general_qual.group(0), re.DOTALL
            )

            if gen_youth_match:
                gen_youth_section_text = gen_youth_match.group(0)

            # ì¼ë°˜ê³µê¸‰ ì‹ í˜¼ë¶€ë¶€ê³„ì¸µ ì‹ ì²­ìê²©
            gen_newlywed_section_pattern = r"(?:2\)|â‘¡)\s*(?:\(ì˜ˆë¹„\))?\s*ì‹ í˜¼ë¶€ë¶€.*?ì‹ ì²­ìê²©.*?(?=(?:3\))\s*.*?ì„ ì •|ì œì¶œì„œë¥˜|$)"
            gen_newlywed_match = re.search(
                gen_newlywed_section_pattern, general_qual.group(0), re.DOTALL
            )

            if gen_newlywed_match:
                gen_newlywed_section_text = gen_newlywed_match.group(0)

        specs = {
            "age": r"â‘ \s*(.*?)(?=â‘¡)",
            "marriage": r"â‘¡\s*(.*?)(?=â‘¢)",
            "household": r"â‘¢\s*(.*?)(?=â‘£)",
            "earnings": r"â‘£.*?(â€»\s*(?:í•´ë‹¹ ì„¸ëŒ€ì˜|ì„¸ëŒ€êµ¬ì„±ì›).*?|.*?ìš”ê±´ ì—†ìŒ.*?)(?=â€»|â‘¤)",
            "car": r"â‘¤\s*(.*?)(?=â‘¥|\n\s*-)",
            "asset": r"â‘¥\s*(.*?)(?=$|\n\s*-|\n\s*â€»|ì‹ í˜¼ë¶€ë¶€ëŠ”)",
        }

        spec_youth_criteria = {}
        spec_newlywed_criteria = {}
        gen_youth_criteria = {}
        gen_newlywed_criteria = {}
        for key, pattern in specs.items():
            item_match = re.search(pattern, spec_youth_section_text, re.DOTALL)
            if item_match:
                clean_text = self._clean_text(item_match.group(1))
                spec_youth_criteria[key] = clean_text
            else:
                spec_youth_criteria[key] = "ë‚´ìš© ì—†ìŒ"

            item_match = re.search(pattern, spec_newlywed_section_text, re.DOTALL)
            if item_match:
                clean_text = self._clean_text(item_match.group(1))
                spec_newlywed_criteria[key] = clean_text
            else:
                spec_newlywed_criteria[key] = "ë‚´ìš© ì—†ìŒ"

            item_match = re.search(pattern, gen_youth_section_text, re.DOTALL)
            if item_match:
                clean_text = self._clean_text(item_match.group(1))
                gen_youth_criteria[key] = clean_text
            else:
                gen_youth_criteria[key] = "ë‚´ìš© ì—†ìŒ"

            item_match = re.search(pattern, gen_newlywed_section_text, re.DOTALL)
            if item_match:
                clean_text = self._clean_text(item_match.group(1))
                gen_newlywed_criteria[key] = clean_text
            else:
                gen_newlywed_criteria[key] = "ë‚´ìš© ì—†ìŒ"

        # íŠ¹ë³„ê³µê¸‰ ì„ ì • ê¸°ì¤€
        ranks = self._parse_selection_criteria(full_text)

        results = {
            "íŠ¹ë³„ê³µê¸‰": {
                "ì²­ë…„ê³„ì¸µ": spec_youth_criteria,
                "ì‹ í˜¼ë¶€ë¶€ê³„ì¸µ": spec_newlywed_criteria,
                "ì„ ì •ê¸°ì¤€": ranks,
            },
            "ì¼ë°˜ê³µê¸‰": {
                "ì²­ë…„ê³„ì¸µ": gen_youth_criteria,
                "ì‹ í˜¼ë¶€ë¶€ê³„ì¸µ": gen_newlywed_criteria,
            },
        }

        return results

    def _parse_selection_criteria(self, full_text):
        income_block_pattern = r"(?:1\)|â‘ )\s*ì†Œë“.*?ìˆœìœ„.*?(?=(?:2\)|â‘¡))"
        location_block_pattern = r"(?:2\)|â‘¡)\s*ì§€ì—­.*?ìˆœìœ„.*?(?=$|\n\s*â€»|\n\*)"

        income_match = re.search(income_block_pattern, full_text, re.DOTALL)
        location_match = re.search(location_block_pattern, full_text, re.DOTALL)

        income_text = income_match.group(0) if income_match else ""
        location_text = location_match.group(0) if location_match else ""

        def extract_ranks(text_block):
            ranks = {}
            rank_pattern = r"-\s*(\d+)ìˆœìœ„\s*[:ï¼š]\s*(.*?)(?=(?:-\s*\d+ìˆœìœ„)|$)"

            matches = re.findall(rank_pattern, text_block, re.DOTALL)
            for rank_num, content in matches:
                clean_content = re.sub(r"\s+", " ", content.strip())
                ranks[f"{rank_num}ìˆœìœ„"] = clean_content

            return ranks

        return {
            "ì†Œë“ê¸°ì¤€": extract_ranks(income_text),
            "ì§€ì—­ê¸°ì¤€": extract_ranks(location_text),
        }

    def _find_target_tables(self, pdf):
        # ë³´ì¦ê¸ˆ, íƒ€ì…, ìœ í˜• í‚¤ì›Œë“œë¡œ í‘œ íƒì§€
        target_tables = []
        for page in pdf.pages:
            # 1. í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  í‘œ ì°¾ê¸° (ê°ì²´ í˜•íƒœë¡œ ì°¾ìŒ)
            tables = page.find_tables()

            for table_obj in tables:
                # 2. í‘œ ë°ì´í„° ì¶”ì¶œ
                table_data = table_obj.extract()

                if not table_data:
                    continue

                # í—¤ë” ê²€ì‚¬ (ë³´ì¦ê¸ˆ/íƒ€ì… í‚¤ì›Œë“œ í™•ì¸)
                header_text = " ".join(filter(None, table_data[0]))
                if "ë³´ì¦ê¸ˆ" in header_text and (
                    "íƒ€ì…" in header_text or "ìœ í˜•" in header_text
                ):

                    bbox_above = (
                        0,  # x0 (ì™¼ìª½ ë)
                        max(0, table_obj.bbox[1] - 40),  # y0 (í‘œ ìƒë‹¨ - 40pt)
                        page.width,  # x1 (ì˜¤ë¥¸ìª½ ë)
                        table_obj.bbox[1],  # y1 (í‘œ ìƒë‹¨)
                    )

                    text_above = page.crop(bbox_above).extract_text() or ""
                    text_above = text_above.replace("\n", " ")  # í•œ ì¤„ë¡œ í•©ì¹˜ê¸°

                    # 4. ì»¨í…ìŠ¤íŠ¸ì—ì„œ 'ê³µê¸‰ ìœ í˜•' í‚¤ì›Œë“œ ì°¾ê¸°
                    supply_context = "ì•Œìˆ˜ì—†ìŒ"
                    if "íŠ¹ë³„ê³µê¸‰" in text_above:
                        supply_context = "íŠ¹ë³„ê³µê¸‰"
                    elif "ì¼ë°˜ê³µê¸‰" in text_above:
                        supply_context = "ì¼ë°˜ê³µê¸‰"

                    # ê²°ê³¼ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                    target_tables.append((table_data, page, supply_context))

        return target_tables

    def _clean_text(self, text):
        """None ì²˜ë¦¬, ê°œí–‰ë¬¸ìì™€ ì‰¼í‘œ ì œê±°í•˜ëŠ” í•¨ìˆ˜"""
        if text is None:
            return ""
        return str(text).replace("\n", " ").replace(",", "").strip()

    def _get_percent_from_header(self, header_cell):
        """í—¤ë” ì…€ì—ì„œ 'XX%' í˜•íƒœì˜ ë¬¸ìì—´ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
        match = re.search(r"(\d+)%", self._clean_text(header_cell))
        if match:
            return match.group(0)
        return "N/A"

    def _to_numeric(self, val, divisor=1.0):
        """ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ê³ , ë‹¨ìœ„ë¥¼ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜"""
        # ì‰¼í‘œ ë¨¼ì € ì œê±°
        cleaned_val = re.sub(r"[^0-9.]", "", str(val))
        if not cleaned_val:
            return None
        try:
            return float(cleaned_val) / divisor
        except ValueError:
            return None

    def _preprocess_header(self, table):
        if not table or len(table) < 2:
            return table[0]

        row0 = table[0]
        row1 = table[1]

        row1_text = " ".join([str(cell) for cell in row1 if cell])
        if "ë³´ì¦ê¸ˆ" not in row1_text and "ì„ëŒ€ë£Œ" not in row1_text:
            return row0

        combined_header = []
        last_valid_header = ""

        for i in range(len(row0)):
            val0 = self._clean_text(row0[i])
            val1 = self._clean_text(row1[i])

            if val0:
                last_valid_header = val0

            merged_text = f"{last_valid_header} {val1}".strip()
            combined_header.append(merged_text)

        return combined_header

    def _analyze_header(self, header):
        """
        í‘œì˜ í—¤ë”(ì²« ë²ˆì§¸ í–‰)ë¥¼ ë¶„ì„í•˜ì—¬ 'ì„¤ê³„ë„(Schema)'ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        - 'íƒ€ì…', 'ê³µê¸‰ìœ í˜•' ì»¬ëŸ¼ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        - (ë³´ì¦ê¸ˆ, ì„ëŒ€ë£Œ) ìŒì˜ ì¸ë±ìŠ¤ì™€ '%' ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        """
        header_info = {
            "type_col": None,
            "supply_col": [],
            "pairs": [],  # (ë³´ì¦ê¸ˆ_idx, ì„ëŒ€ë£Œ_idx, "ë³´ì¦ê¸ˆ%") íŠœí”Œì„ ì €ì¥
        }

        cleaned_header = [self._clean_text(h) for h in header]

        # íƒ€ì…/ê³µê¸‰ ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
        for i, text in enumerate(cleaned_header):
            # 'íƒ€ì…' ë˜ëŠ” 'ì „ìš©'ì´ í¬í•¨ëœ ê°€ì¥ êµ¬ì²´ì ì¸ ì»¬ëŸ¼ì„ ì°¾ìŒ
            if "íƒ€ì…" in text or "ì „ìš©" in text:
                header_info["type_col"] = i
                break

        type_idx = (
            header_info["type_col"]
            if header_info["type_col"] is not None
            else len(header)
        )

        for i in range(type_idx):
            text = cleaned_header[i]

            if "í˜¸ìˆ˜" in text:
                continue

            if any(k in text for k in ["ìœ í˜•", "ëŒ€ìƒ", "ê³„ì¸µ"]):
                header_info["supply_col"].append(i)

            elif text == "" and (i - 1) in header_info["supply_col"]:
                header_info["supply_col"].append(i)

        # 2. ë³´ì¦ê¸ˆ/ì„ëŒ€ë£Œ ìŒ(Pair) ì°¾ê¸°
        i = 0
        while i < len(cleaned_header) - 1:
            cell_text = cleaned_header[i]
            next_cell_text = cleaned_header[i + 1]

            # (ë³´ì¦ê¸ˆ, ì„ëŒ€ë£Œ) íŒ¨í„´ íƒì§€
            if "ë³´ì¦ê¸ˆ" in cell_text and "ì„ëŒ€ë£Œ" in next_cell_text:
                percent = self._get_percent_from_header(cell_text)
                header_info["pairs"].append((i, i + 1, percent))
                i += 2
            else:
                i += 1
        return header_info

    def _detect_units(self, page):
        page_text = page.extract_text() or ""

        if "(ë‹¨ìœ„ : ì›)" in page_text:
            return 10000.0
        if "(ë‹¨ìœ„ : ì²œì›)" in page_text:
            return 10.0
        if "(ë‹¨ìœ„ : ë§Œì›)" in page_text:
            return 1.0

        return 10000.0

    def _extract_rows(
        self, table, header_info, unit_divisor, file_name, supply_context
    ):
        current_supply_type = supply_context if supply_context != "ì•Œìˆ˜ì—†ìŒ" else ""

        start_row_index = 1
        row1_text = " ".join([str(c) for c in table[1] if c])
        if "ì„ëŒ€ë£Œ" in row1_text or "ë³´ì¦ê¸ˆ" in row1_text:
            start_row_index = 2

        data_rows = table[start_row_index:]

        type_idx = header_info["type_col"]
        supply_idxs = header_info["supply_col"]

        if type_idx is None:
            print(f"   âš ï¸ 'íƒ€ì…' ë˜ëŠ” 'ì „ìš©' ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•´ í‘œ íŒŒì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        last_supply_value = ""
        for row in data_rows:
            cleaned_first_cell = self._clean_text(row[0]).replace(" ", "")
            if "í•©ê³„" in cleaned_first_cell or "ì†Œê³„" in cleaned_first_cell:
                continue

            split_row = [self._clean_text(cell) for cell in row]
            row_text_joined = " ".join(split_row)

            if "íŠ¹ë³„" in row_text_joined:
                current_supply_type = "íŠ¹ë³„ê³µê¸‰"
            elif "ì¼ë°˜" in row_text_joined:
                current_supply_type = "ì¼ë°˜ê³µê¸‰"

            if split_row[type_idx] is None or split_row[type_idx] == "":
                continue

            supply_val = " ".join(
                [split_row[idx] for idx in supply_idxs if idx is not None]
            )

            if supply_val:
                if any(
                    k in supply_val.replace(" ", "")
                    for k in ["ì²­ë…„ë˜ëŠ”ì‹ í˜¼ë¶€ë¶€", "ì²­ë…„ì‹ í˜¼ë¶€ë¶€", "ì²­ë…„â¦ì‹ í˜¼ë¶€ë¶€"]
                ):
                    last_supply_value = "ì²­ë…„ ë˜ëŠ” ì‹ í˜¼ë¶€ë¶€"
                elif "ì²­ë…„" in supply_val.replace(" ", ""):
                    last_supply_value = "ì²­ë…„"
                elif "ì‹ í˜¼ë¶€ë¶€" in supply_val.replace(" ", ""):
                    last_supply_value = "ì‹ í˜¼ë¶€ë¶€"
                else:
                    last_supply_value = supply_val
            supply_val = last_supply_value

            for deposit_idx, rent_idx, percent in header_info["pairs"]:
                deposit_val = self._to_numeric(
                    split_row[deposit_idx],
                    unit_divisor,
                )
                rent_val = self._to_numeric(
                    split_row[rent_idx],
                    unit_divisor,
                )

                if deposit_val is None and rent_val is None:
                    continue
                if "ê³µê¸‰" in supply_val:
                    continue

                record = {
                    "ê³µê¸‰ìœ í˜•1": current_supply_type,
                    "ê³µê¸‰ìœ í˜•2": supply_val,
                    "íƒ€ì…": split_row[type_idx],
                    "ë³´ì¦ê¸ˆ(ë§Œì›)": deposit_val,
                    "ì„ëŒ€ë£Œ(ë§Œì›)": rent_val,
                    "ë³´ì¦ê¸ˆ%": percent,
                }

                self.all_data.append(record)


DB_CONFIG = {
    "host": "century20-rds.clqcgo84gd3x.us-west-2.rds.amazonaws.com",
    "user": "admin",
    "password": "century20!",
    "db": "century20",
    "port": 3306,
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor,  # SELECT ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°›ê¸° ìœ„í•´ í•„ìˆ˜
}


def main():
    start_time = datetime.now()
    log("=" * 80, "INFO")
    log(f"ğŸš€ New Extractor ì‹œì‘", "INFO")
    log(f"   ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}", "INFO")
    log("=" * 80, "INFO")
    
    parser = AnsimJutaekParser()
    conn = None

    try:
        log("ğŸ“Š DB ì—°ê²° ì‹œë„ ì¤‘...", "INFO")
        log(f"   í˜¸ìŠ¤íŠ¸: {DB_CONFIG['host']}", "DEBUG")
        log(f"   ë°ì´í„°ë² ì´ìŠ¤: {DB_CONFIG['database']}", "DEBUG")
        
        conn = pymysql.connect(**DB_CONFIG)
        log("âœ… DB ì—°ê²° ì„±ê³µ", "INFO")

        cursor = conn.cursor()

        select_sql = "SELECT * FROM Announcements WHERE listing_number is not null order by listing_number desc limit 5"
        log(f"ğŸ” ì¿¼ë¦¬ ì‹¤í–‰: {select_sql}", "DEBUG")

        cursor.execute(select_sql)
        rows = cursor.fetchall()
        log(f"ğŸ“‹ ì´ {len(rows)}ê°œì˜ ì²˜ë¦¬ ëŒ€ìƒ ë°œê²¬", "INFO")
        
        if len(rows) == 0:
            log("âš ï¸ ì²˜ë¦¬í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.", "WARN")
            return

        success_count = 0
        fail_count = 0

        log("ğŸ¤– ML ëª¨ë¸ ë¡œë”© ì¤‘...", "INFO")
        model = load_model_assets()
        log("âœ… ML ëª¨ë¸ ë¡œë”© ì™„ë£Œ", "INFO")
        log("-" * 80, "INFO")

        for idx, row in enumerate(rows, 1):
            announcement_id = row["announcement_id"]
            title = row.get("title", "ì œëª© ì—†ìŒ")
            listing_number = row.get("listing_number", "N/A")
            
            log(f"[{idx}/{len(rows)}] ì²˜ë¦¬ ì‹œì‘", "INFO")
            log(f"   ğŸ“Œ ID: {announcement_id}", "INFO")
            log(f"   ğŸ“Œ Listing Number: {listing_number}", "INFO")
            log(f"   ğŸ“Œ ì œëª©: {title[:50]}{'...' if len(title) > 50 else ''}", "INFO")
            
            try:
                # 1. íŒŒì„œ ì‹¤í–‰
                log(f"   [1/3] PDF íŒŒì‹± ì‹œì‘...", "DEBUG")
                parser.update_row(row, cursor)
                log(f"   [1/3] âœ… PDF íŒŒì‹± ì™„ë£Œ", "DEBUG")
                
                # 2. ê°€ê²© ì˜ˆì¸¡
                log(f"   [2/3] ê°€ê²© ì˜ˆì¸¡ ì‹œì‘...", "DEBUG")
                updated_price_list = preprocess_and_predict_group(row, model)
                
                if updated_price_list:
                    log(f"   [2/3] âœ… ê°€ê²© ì˜ˆì¸¡ ì™„ë£Œ: {len(updated_price_list)}ê°œ í•­ëª©", "DEBUG")
                    
                    # 3. DB ì—…ë°ì´íŠ¸
                    log(f"   [3/3] DB ì—…ë°ì´íŠ¸ ì‹œì‘...", "DEBUG")
                    new_json_str = json.dumps(updated_price_list, ensure_ascii=False)
                    update_sql = "UPDATE Announcements SET price = %s WHERE announcement_id = %s"
                    cursor.execute(update_sql, (new_json_str, announcement_id))
                    log(f"   [3/3] âœ… DB ì—…ë°ì´íŠ¸ ì™„ë£Œ", "DEBUG")
                else:
                    log(f"   [2/3] âš ï¸ ê°€ê²© ì˜ˆì¸¡ ê²°ê³¼ ì—†ìŒ", "WARN")
                
                success_count += 1
                log(f"   âœ… ê³µê³  ì²˜ë¦¬ ì„±ê³µ", "INFO")
                
            except Exception as e:
                fail_count += 1
                log(f"   âŒ ê³µê³  ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", "ERROR")
                import traceback
                log(f"   ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}", "ERROR")
            
            log("-" * 80, "INFO")

        log("ğŸ’¾ DB ì»¤ë°‹ ì‹œì‘...", "INFO")
        conn.commit()
        log("âœ… DB ì»¤ë°‹ ì™„ë£Œ", "INFO")
        
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()
        
        log("=" * 80, "INFO")
        log("ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!", "INFO")
        log(f"   âœ… ì„±ê³µ: {success_count}ê±´", "INFO")
        log(f"   âŒ ì‹¤íŒ¨: {fail_count}ê±´", "INFO")
        log(f"   â±ï¸  ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ", "INFO")
        log(f"   ğŸ• ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}", "INFO")
        log("=" * 80, "INFO")

    except Exception as e:
        log("=" * 80, "ERROR")
        log(f"ğŸ’¥ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {str(e)}", "ERROR")
        import traceback
        log(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}", "ERROR")
        log("=" * 80, "ERROR")
        
        if conn:
            log("âª íŠ¸ëœì­ì…˜ ë¡¤ë°± ì‹œë„...", "WARN")
            try:
                conn.rollback()
                log("âœ… ë¡¤ë°± ì™„ë£Œ", "INFO")
            except Exception as rollback_err:
                log(f"âŒ ë¡¤ë°± ì‹¤íŒ¨: {rollback_err}", "ERROR")

    finally:
        if conn:
            log("ğŸ”Œ DB ì—°ê²° ì¢…ë£Œ ì¤‘...", "INFO")
            conn.close()
            log("âœ… DB ì—°ê²° ì¢…ë£Œ ì™„ë£Œ", "INFO")


if __name__ == "__main__":
    main()
